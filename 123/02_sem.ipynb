{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gunakemm/Jupyter/blob/main/123/02_sem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4wEZLgDamkj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2cska53amkl",
        "outputId": "dcda5bb6-8fa5-41b8-e85a-6414aa83f8c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6032</th>\n",
              "      <td>[{'name': 'Ashkan Esmaeili'}, {'name': 'Farokh...</td>\n",
              "      <td>12</td>\n",
              "      <td>1606.03672v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>In this paper, we will investigate the efficac...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Comparison of Several Sparse Recovery Methods ...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22405</th>\n",
              "      <td>[{'name': 'Md ashad Alam'}, {'name': 'Osamu Ko...</td>\n",
              "      <td>1</td>\n",
              "      <td>1606.00118v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>In genome-wide interaction studies, to detect ...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Gene-Gene association for Imaging Genetics Dat...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21220</th>\n",
              "      <td>[{'name': 'Wei Gao'}, {'name': 'David Hsu'}, {...</td>\n",
              "      <td>16</td>\n",
              "      <td>1710.05627v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>10</td>\n",
              "      <td>How can a delivery robot navigate reliably to ...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Intention-Net: Integrating Planning and Deep L...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>[{'name': 'Richard J. Preen'}, {'name': 'Larry...</td>\n",
              "      <td>18</td>\n",
              "      <td>1204.4200v2</td>\n",
              "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org...</td>\n",
              "      <td>4</td>\n",
              "      <td>A number of representation schemes have been p...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Discrete Dynamical Genetic Programming in XCS</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>[{'name': 'Jacob Andreas'}, {'name': 'Marcus R...</td>\n",
              "      <td>9</td>\n",
              "      <td>1511.02799v4</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>11</td>\n",
              "      <td>Visual question answering is fundamentally com...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Neural Module Networks</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "6032   [{'name': 'Ashkan Esmaeili'}, {'name': 'Farokh...   12  1606.03672v1   \n",
              "22405  [{'name': 'Md ashad Alam'}, {'name': 'Osamu Ko...    1  1606.00118v1   \n",
              "21220  [{'name': 'Wei Gao'}, {'name': 'David Hsu'}, {...   16  1710.05627v2   \n",
              "767    [{'name': 'Richard J. Preen'}, {'name': 'Larry...   18   1204.4200v2   \n",
              "125    [{'name': 'Jacob Andreas'}, {'name': 'Marcus R...    9  1511.02799v4   \n",
              "\n",
              "                                                    link  month  \\\n",
              "6032   [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
              "22405  [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
              "21220  [{'rel': 'alternate', 'href': 'http://arxiv.or...     10   \n",
              "767    [{'rel': 'related', 'href': 'http://dx.doi.org...      4   \n",
              "125    [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
              "\n",
              "                                                 summary  \\\n",
              "6032   In this paper, we will investigate the efficac...   \n",
              "22405  In genome-wide interaction studies, to detect ...   \n",
              "21220  How can a delivery robot navigate reliably to ...   \n",
              "767    A number of representation schemes have been p...   \n",
              "125    Visual question answering is fundamentally com...   \n",
              "\n",
              "                                                     tag  \\\n",
              "6032   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "22405  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "21220  [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n",
              "767    [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n",
              "125    [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "\n",
              "                                                   title  year  \n",
              "6032   Comparison of Several Sparse Recovery Methods ...  2016  \n",
              "22405  Gene-Gene association for Imaging Genetics Dat...  2016  \n",
              "21220  Intention-Net: Integrating Planning and Deep L...  2017  \n",
              "767        Discrete Dynamical Genetic Programming in XCS  2012  \n",
              "125                               Neural Module Networks  2015  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_json('./arxivData.json')\n",
        "data.sample(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yOINhVgamkm",
        "outputId": "f8514dd1-3172-46be-b61f-ea7cba299b15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.loc[0, 'summary']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spm73YRkamkm",
        "outputId": "7a348f10-9070-4b60-920a-cabbe8d0008a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
              " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
              " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace('\\n', ' '), axis=1).to_list()\n",
        "\n",
        "sorted(lines, key=len)[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pj9URYOamkn"
      },
      "outputs": [],
      "source": [
        "from nltk import WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "lines = [' '.join(tokens for tokens in tokenizer.tokenize(line.lower())) for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUUsE06Iamkn"
      },
      "outputs": [],
      "source": [
        "assert sorted(lines, key=len)[0] == 'differential contrastive divergence ; this paper has been retracted .'\n",
        "assert sorted(lines, key=len)[2] == 'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSfgJSCpamko"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# special tokens:\n",
        "# - 'UNK' represents absent tokens,\n",
        "# - 'EOS' is a special token after the end of sequence\n",
        "\n",
        "UNK, EOS = '_UNK_', '_EOS_'\n",
        "\n",
        "def count_ngrams(lines, n):\n",
        "    counts = defaultdict(Counter)\n",
        "    for line in lines:\n",
        "        tokens = tokenizer.tokenize(line)\n",
        "        tokens = [UNK] * (n - 1) + tokens\n",
        "        tokens.append(EOS)\n",
        "\n",
        "        for idx in range(n - 1, len(tokens)):\n",
        "            context = tuple(tokens[idx - n + 1: idx])\n",
        "            next_token = tokens[idx]\n",
        "            counts[context][next_token] += 1\n",
        "\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I0_uHSUamko"
      },
      "outputs": [],
      "source": [
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
        "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
        "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
        "assert dummy_counts['p', '=']['np'] == 2\n",
        "assert dummy_counts['author', '.']['_EOS_'] == 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu0x3CS7amkp"
      },
      "outputs": [],
      "source": [
        "class NGramLanguageModel:\n",
        "    def __init__(self, lines, n):\n",
        "        \"\"\"\n",
        "        Train a simple count-based language model:\n",
        "        compute probabilities P(w_t | prefix) given ngram counts\n",
        "\n",
        "        :param n: computes probability of next token given (n - 1) previous words\n",
        "        :param lines: an iterable of strings with space-separated tokens\n",
        "        \"\"\"\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "\n",
        "        # compute token proabilities given counts\n",
        "        self.probs = defaultdict(Counter)\n",
        "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
        "        for key, counter in counts.items():\n",
        "            for val, count in counter.items():\n",
        "                self.probs[key][val] = count / sum(counter.values())\n",
        "        # populate self.probs with actual probabilities\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
        "        \"\"\"\n",
        "        prefix = prefix.split()\n",
        "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
        "        prefix = [UNK] * (self.n - 1 - len(prefix)) + prefix\n",
        "        return self.probs[tuple(prefix)]\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :param next_token: the next token to predict probability for\n",
        "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
        "        \"\"\"\n",
        "        return self.get_possible_next_tokens(prefix).get(next_token, 0)\n",
        "\n",
        "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnSgYwCeamkp"
      },
      "outputs": [],
      "source": [
        "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
        "assert np.allclose(p_initial['learning'], 0.02)\n",
        "assert np.allclose(p_initial['a'], 0.13)\n",
        "assert np.allclose(p_initial.get('meow', 0), 0)\n",
        "assert np.allclose(sum(p_initial.values()), 1)\n",
        "\n",
        "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
        "assert np.allclose(p_a['machine'], 0.15384615)\n",
        "assert np.allclose(p_a['note'], 0.23076923)\n",
        "assert np.allclose(p_a.get('the', 0), 0)\n",
        "assert np.allclose(sum(p_a.values()), 1)\n",
        "\n",
        "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
        "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
        "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
        "    \"your 3-gram model should only depend on 2 previous words\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B1WbEnJamkq"
      },
      "outputs": [],
      "source": [
        "lm = NGramLanguageModel(lines=lines, n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbqgQxz8amkq",
        "outputId": "8e26256f-4e26-4b1e-c825-0b38885e4e5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.999999999999975"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probs = np.array(list(lm.get_possible_next_tokens('a').values()))\n",
        "probs = probs ** (1 / 2) / sum(probs)\n",
        "\n",
        "total_probs = sum(probs)\n",
        "\n",
        "probs = probs / total_probs\n",
        "sum(probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6YyGVoramkr"
      },
      "outputs": [],
      "source": [
        "def get_next_token(lm, prefix, temperature=1.0):\n",
        "\n",
        "    if temperature == 0:\n",
        "        return max(lm.get_possible_next_tokens(prefix), key=lm.get_possible_next_tokens(prefix).get)\n",
        "\n",
        "    tokens = np.array(list(lm.get_possible_next_tokens(prefix).keys()))\n",
        "    probs = np.array(list(lm.get_possible_next_tokens(prefix).values()))\n",
        "    probs = probs ** (1 / temperature) / sum(probs)\n",
        "\n",
        "    total_prob = sum(probs)\n",
        "\n",
        "    probs = probs / total_prob\n",
        "\n",
        "    next_token = np.random.choice(tokens, p=probs)\n",
        "\n",
        "    return next_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNncfbT8amkr",
        "outputId": "96370559-1a36-4bc2-95ee-ff10f06bc0fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks nice!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
        "assert 250 < test_freqs['not'] < 450\n",
        "assert 8500 < test_freqs['been'] < 9500\n",
        "assert 1 < test_freqs['lately'] < 200\n",
        "\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
        "assert 1500 < test_freqs['learning'] < 3000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
        "assert 8000 < test_freqs['learning'] < 9000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
        "assert test_freqs['learning'] == 10000\n",
        "\n",
        "print(\"Looks nice!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c4MiLCeamkr",
        "outputId": "98c2db0d-851c-4fea-8940-df6d78332ae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "function driven diffusion for personalized counterfactual inference ; variational inference that can then help derive higher level of syntactic trees . we apply our semi - supervised setting , based on ontology have emerged as a part of our model first adjusts each word is linked to local columbus receptors . running the base mean level of abstraction to represent the results suggest that a specific task of novelty , rather than isolated object instances in terms of accuracy . _EOS_\n"
          ]
        }
      ],
      "source": [
        "prefix = 'function'\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjAadJOhamkr",
        "outputId": "91bea25f-7ea6-4739-8012-deaeaa6a3ca8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13.368554817277525"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
        "\n",
        "    total_log_prob = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = tokenizer.tokenize(line.lower())\n",
        "        tokens.append(EOS)\n",
        "\n",
        "        total_tokens += len(tokens)\n",
        "\n",
        "        sentence_log_prob = 0.0\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            context = tokens[:i]\n",
        "            context_str = ' '.join(context) if context else ''\n",
        "\n",
        "            prob = lm.get_next_token_prob(context_str, tokens[i])\n",
        "\n",
        "            if prob <= 0:\n",
        "                log_prob = min_logprob\n",
        "            else:\n",
        "                log_prob = np.log(prob)\n",
        "\n",
        "                if log_prob < min_logprob:\n",
        "                    log_prob = min_logprob\n",
        "\n",
        "            sentence_log_prob += log_prob\n",
        "\n",
        "        total_log_prob += sentence_log_prob\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    perplexity = np.exp(-total_log_prob / total_tokens)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "perplexity(lm, dummy_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEMDEC2vamkr",
        "outputId": "2998d4ff-38ca-4240-cd4d-0aba9ebda3db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"
          ]
        }
      ],
      "source": [
        "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
        "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
        "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
        "\n",
        "ppx1 = perplexity(lm1, dummy_lines)\n",
        "ppx3 = perplexity(lm3, dummy_lines)\n",
        "ppx10 = perplexity(lm10, dummy_lines)\n",
        "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
        "\n",
        "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
        "\n",
        "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be non-negative and reasonably small\"\n",
        "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
        "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
        "    \" Make sure you use min_logprob right\"\n",
        "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-7jZEl5amkr",
        "outputId": "616c84f3-107d-4073-ca58-a4c75b937fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 1, Perplexity = 1832.23136\n",
            "N = 2, Perplexity = 85653987.28543\n",
            "N = 3, Perplexity = 61999196239911532363776.00000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "for n in (1, 2, 3):\n",
        "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZAaxB4uamkr"
      },
      "outputs": [],
      "source": [
        "class LaplaceLanguageModel(NGramLanguageModel):\n",
        "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        for prefix in counts:\n",
        "            token_counts = counts[prefix]\n",
        "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
        "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
        "                                          for token in token_counts}\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        if next_token in token_probs:\n",
        "            return token_probs[next_token]\n",
        "        else:\n",
        "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
        "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjYJv631amks"
      },
      "outputs": [],
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
        "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dqxEkLHamks"
      },
      "outputs": [],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScXgFT2Eamks"
      },
      "outputs": [],
      "source": [
        "class KneserNeyLanguageModel(NGramLanguageModel):\n",
        "\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        counts = {}\n",
        "        for i in range(1, self.n):\n",
        "            counts[i] = count_ngrams(lines, i)\n",
        "\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        for prefix in counts:\n",
        "            token_counts = counts[prefix]\n",
        "            total_count = sum(token_counts.values())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        token_probs = super().get_next_token_prob(prefix, next_token)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}